{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_file = \"data/es.txt\"\n",
    "target_file = \"data/en.txt\"\n",
    "source_lines = list(open(source_file, \"r\"))\n",
    "target_lines = list(open(target_file, \"r\"))\n",
    "\n",
    "# very crude\n",
    "s=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "training_pairs = zip(source_lines, target_lines)\n",
    "\n",
    "max_seq_len = 30\n",
    "keep_pair_if = lambda pair: len(pair[0].split(\" \")) < max_seq_len and len(pair[1].split(\" \")) < max_seq_len\n",
    "\n",
    "training_pairs = filter(keep_pair_if, training_pairs)\n",
    "source_lines, target_lines = zip(*training_pairs)\n",
    "\n",
    "class Corpus():\n",
    "    def __init__(self, input_lines, n_train=5000):\n",
    "        self.SOS = 0\n",
    "        self.EOS = 1\n",
    "        self.idx_word, self.word_idx = self.parse_words(input_lines)\n",
    "        self.n_train = n_train\n",
    "        \n",
    "        self.parse_words(input_lines)\n",
    "        self.corpus_size = len(self.idx_word)\n",
    "        self.lines = [l.strip().lower() for l in input_lines]\n",
    "        self.training = [self.sentence_to_index(l) for l in self.lines]\n",
    "        \n",
    "    def parse_words(self, lines):\n",
    "        sls = lambda s: s.strip().lower().split(\" \")\n",
    "        words = [\"<SOS>\", \"<EOS>\"] + sorted(set(list(chain(*[sls(l) for l in lines]))))\n",
    "        idx_word = dict(list(enumerate(words)))\n",
    "        word_idx = dict(zip(words, list(range(len(words)))))\n",
    "        \n",
    "        return idx_word, word_idx\n",
    "    \n",
    "    def sentence_to_index(self, s):\n",
    "        words = s.split(\" \")\n",
    "        indices = [self.word_idx[word] for word in words]\n",
    "        return indices\n",
    "    \n",
    "    def index_to_sentence(self, indices):\n",
    "        return \" \".join(self.idx_word[idx] for idx in indices)\n",
    "\n",
    "    \n",
    "# you will use this class later, consider using ipython terminal\n",
    "# to get used to initializing Parameters, Variables\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # use rand(n) to get tensors to intitialize your weight matrix and bias tensor \n",
    "        # then use Parameter( ) to wrap them as Variables visible to module.parameters()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_var, use_relu=False):\n",
    "        # standard linear layer, just an affine transform with no nonlinearity \n",
    "        #  for this lab\n",
    "        # use torch.matmul not mm\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encoder topology\n",
    "-standard GRU, embed each word before submitting it.\n",
    "-use GRUCell not GRU\n",
    "\n",
    "Never use one hot encodings! Your input tensors to the GRU should have (1,1,hidden) size\n",
    "-pytorch typically works with 3d (seq_len, batch_size, hidden_dim) tensors, unlike tensorflow's list of tensors.\n",
    "\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, source_vocab_size, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = source_vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embed = nn.Embedding() # needs parameters\n",
    "        #self.GRUCell = nn.GRUCell() # needs parameters\n",
    "                \n",
    "    def forward(self, input_variable):\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\"\"\"\n",
    "GRU is initialized to the number of layers\n",
    "-run it one time step at a time using tensors of shape (1,1,hidden_size)\n",
    "-use zero's as the initial hidden state\n",
    "\n",
    "Use teacher forcing to initially establish word-word connections\n",
    "-recommend around .5 to .7\n",
    "\n",
    "without teacher forcing\n",
    "next_input = tensor([[SOS]])\n",
    "next_hidden = hidden\n",
    "for i in 0..len(input_sequence):\n",
    "  embed(next_input), h_i-1 -> GRU -> output, h_i\n",
    "  output[-1] -> LinearLayer (to number of words in English corpus) -> SoftMax -> probabilities\n",
    "  probabilities -> argmax -> next_input\n",
    "  if next_input = EOS:\n",
    "    break\n",
    "\n",
    "with teacher forcing, helps to form one to one connections between words \n",
    "embedded = embed(reference_var)\n",
    "next_hidden = hidden\n",
    "for i in 0..len(embedded):\n",
    "  embedded[i], h_i-1 -> GRU -> output, h_i\n",
    "  output[-1] -> LinearLayer (to number of words in English corpus) -> SoftMax -> probabilities\n",
    "\n",
    "Return the probabilities (for the loss) and predictions (for printing and easy later evaluation) whether or not using teacher forcing.\n",
    "\"\"\"\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, target_vocab_size, n_layers, max_target_length=30):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.max_target_length = self.max_target_length\n",
    "        \n",
    "        # initialize GRU, a Linear Layer\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, context, target_variable=None, encoder_outputs=None):\n",
    "        use_teacher_forcing = target_variable or None\n",
    "        \n",
    "        # we may give you a module to implement the attention mechanism.        \n",
    "        use_attention = encoder_outputs or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_corpus = Corpus(source_lines)\n",
    "target_corpus = Corpus(target_lines)\n",
    "n_spanish = source_corpus.corpus_size\n",
    "n_english = target_corpus.corpus_size\n",
    "\n",
    "# accumulate gradients batch_size times before updating\n",
    "epoch_length = np.minimum(len(training_pairs), 4000)\n",
    "batch_size = 20 # rather iterations between optimizer updates\n",
    "max_seq_len = 30 # watch the number of sentences drop\n",
    "n_layers = 2\n",
    "learning_rate = .01\n",
    "print_every = batch_size\n",
    "n_epochs = 30\n",
    "\n",
    "input_size = 500\n",
    "hidden_size = 500\n",
    "teacher_forcing_ratio = .5\n",
    "n_test = 5000\n",
    "\n",
    "\n",
    "indexed_pairs = zip(source_corpus.training, target_corpus.training)\n",
    "\n",
    "# support for training yet to come\n",
    "encoder = Encoder(input_size, hidden_size, n_spanish, n_layers)\n",
    "decoder = Decoder(input_size, hidden_size, n_spanish, n_layers, max_seq_len)\n",
    "\n",
    "training_pairs = training_pairs[:-n_test]\n",
    "testing_pairs = training_pairs[-n_test:]\n",
    "\n",
    "def get_loss(output_probs, correct_indices):\n",
    "    \"\"\" params:\n",
    "         output_probs: a list of Variable (Not Tensor)\n",
    "         correct_indices: a list or tensor of type int with the same length \"\"\"\n",
    "    \n",
    "    # You can batch this part if you want by concatenating the output_probs\n",
    "    # convert correct_indices to a (seq_len, 1) Tensor if batching or list of tensors\n",
    "    # Use NLLoss as it takes probabilities\n",
    "    # sanity check: loss should be a Variable\n",
    "\n",
    "def print_output(teacher_forced, source_indices, predicted_indices, reference_indices):\n",
    "    global source_corpus, target_corpus\n",
    "    print(\"Iteration %d\", end=\" \")\n",
    "    if teacher_forced:\n",
    "        print(\"using teacher forcing\")\n",
    "    print (\"In:    \", source_corpus.index_to_sentence(source_indices))\n",
    "    print (\"Out:   \", target_corpus.index_to_sentence(predicted_indices))\n",
    "    print (\"Ground:\", target_corpus.index_to_sentence(predicted_indices))\n",
    "\n",
    "# can combine encoder and decoder\n",
    "def train(encoder, decoder, training_pairs, testing_pairs, \n",
    "                source_corpus, target_corpus, teacher_forcing_ratio, \n",
    "                epoch_size, learning_rate, batch_size, print_every):\n",
    "    \"\"\"\n",
    "    You may want to lower the teacher forcing ratio as the number \n",
    "      of epochs progresses as it starts to learn word-word connections.\n",
    "    \n",
    "    In PyTorch some optimizers don't allow for decaying learning rates\n",
    "    -thankfully initializing new optimizers is trivial\n",
    "    -You may want to use a learning rate schedule instead of decay\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the optimizers\n",
    "    \n",
    "    batched_loss = 0\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(epoch_size):\n",
    "            use_teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # consider whether or not to use teacher forcing on printing iterations\n",
    "            # use_teacher_forcing = use_teacher_forcing or (i % print_every == 0)\n",
    "            \n",
    "            source_var, reference_var = np.random.choice(training_pairs)\n",
    "\n",
    "            # convert source_var and reference_var to Tensors then Variables\n",
    "            # run source_var through the encoder\n",
    "            # if use_teacher_forcing input the target variable to decoder\n",
    "            # compute the loss between the predicted probability distributions and the target indexed tensor\n",
    "\n",
    "            # implement the batch update as shown in the spec\n",
    "\n",
    "            # print results\n",
    "            if i % print_every == 0:\n",
    "                print_output(use_teacher_forcing, source_idc, predicted, target_idc)\n",
    "            \n",
    "    # run test iteration\n",
    "    \n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Help debugging / learning pytorch.\n",
    "Consider using ipython terminal as part of your workflow and to get used to pytorch and the classes used in this lab.\n",
    "pdb may be of interest to you.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
