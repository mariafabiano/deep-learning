{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from numpy.random import randint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    def __init__(self, input_lines, n_train=5000):\n",
    "        self.SOS = 0\n",
    "        self.EOS = 1\n",
    "        self.idx_word, self.word_idx = self.parse_words(input_lines)\n",
    "        self.n_train = n_train\n",
    "        \n",
    "        self.parse_words(input_lines)\n",
    "        self.corpus_size = len(self.idx_word)\n",
    "        self.lines = [l.strip().lower() for l in input_lines]\n",
    "        self.training = [self.sentence_to_index(l) for l in self.lines]\n",
    "        \n",
    "    def parse_words(self, lines):\n",
    "        sls = lambda s: s.strip().lower().split(\" \")\n",
    "        words = [\"<SOS>\", \"<EOS>\"] + sorted(set(list(chain(*[sls(l) for l in lines]))))\n",
    "        idx_word = dict(list(enumerate(words)))\n",
    "        word_idx = dict(zip(words, list(range(len(words)))))\n",
    "        \n",
    "        return idx_word, word_idx\n",
    "    \n",
    "    def sentence_to_index(self, s):\n",
    "        words = s.split(\" \")\n",
    "        indices = [self.word_idx[word] for word in words]\n",
    "        return indices\n",
    "    \n",
    "    def index_to_sentence(self, indices):\n",
    "        return \" \".join(self.idx_word[idx] for idx in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#### old/my own code ####\\nclass Decoder(nn.Module):\\n    def __init__(self, hidden_size, target_vocab_size, n_layers=2, max_target_length=30):\\n        super(Decoder, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.input_size = hidden_size\\n        self.vocab_size = target_vocab_size\\n        self.n_layers = n_layers\\n        self.max_length = self.max_target_length\\n        \\n        # initialize n_layers cells of type nn.GRUCell, an nn.Embedding like before\\n        # initialize a Linear module and nn.ModuleList\\n        \\n    def forward(self, context, target_variable=None):\\n        # if meant to teacher forcing include the target_variable\\n        use_teacher_forcing = target_variable or None\\n        \\n        # return predictions as well for easier sampling'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "encoder topology\n",
    "-standard GRU topology, see slides for a reveiw\n",
    "-see if you can figure it out without looking at the decoder's pseuocode below\n",
    "-context vector is the hidden state of the last time step and last layer\n",
    "-use nn.GRUCell not nn.GRU\n",
    "-use zero Variables as the initial hidden states\n",
    "\n",
    "Notes on RNN workflows in pytorch\n",
    "-Never use one hot encodings in pytorch. It's programmed to use indexed tensors whenever possible\n",
    "-pytorch RNNs typically take (batch, seq_len, hidden_dim) tensors\n",
    "-the result of embedding (batch, seq_len) index tensors of type Long\n",
    "-but like tensorflow and in this lab RNNCell descendents take \n",
    "  (batch, input_dim), (batch, hidden_dim) shaped tensor Variables\n",
    "\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, source_vocab_size, n_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = source_vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embed = nn.Embedding() # needs parameters\n",
    "        #cells = [nn.GRUCell(*parameters) for _ in n_layers]\n",
    "        # instantiate a ModuleList as a class member so your GRUCell-s are visible to \n",
    "        #   encoder.parameters()\n",
    "        \n",
    "    def forward(self, source_variable):        \n",
    "        # code up a vanilla GRU using nn.GRUCell, return the last output\n",
    "        pass\n",
    "    \n",
    "\"\"\"\n",
    "#### old/my own code ####\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, source_vocab_size, n_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = source_vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embed = nn.Embedding() # needs parameters\n",
    "        #cells = [nn.GRUCell(*parameters) for _ in n_layers]\n",
    "        # instantiate a ModuleList as a class member so your GRUCell-s are visible to \n",
    "        #   encoder.parameters()\n",
    "        \n",
    "    def forward(self, source_variable):        \n",
    "        # code up a vanilla GRU using nn.GRUCell, return the last output\n",
    "        pass\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "GRU is initialized to the number of layers\n",
    "-run it one time step at a time using tensors of shape (1,1,hidden_size)\n",
    "-use zero's as the initial hidden state\n",
    "\n",
    "Use teacher forcing to initially establish word-word connections\n",
    "-recommend around .5 to .7\n",
    "\n",
    "without teacher forcing\n",
    "next_input = tensor([[SOS]])\n",
    "next_hidden = hidden\n",
    "for i in 0..len(input_sequence):\n",
    "  embed(next_input), h_i-1 -> GRU -> output, h_i\n",
    "  output[-1] -> LinearLayer (to number of words in English corpus) -> SoftMax -> probabilities\n",
    "  probabilities -> argmax -> next_input\n",
    "  if next_input = EOS:\n",
    "    break\n",
    "\n",
    "with teacher forcing, helps to form one to one connections between words \n",
    "embedded = embed(reference_var)\n",
    "next_hidden = hidden\n",
    "for i in 0..len(embedded):\n",
    "  embedded[i], h_i-1 -> GRU -> output, h_i\n",
    "  output[-1] -> LinearLayer (to number of words in English corpus) -> SoftMax -> probabilities\n",
    "\n",
    "Return the probabilities (for the loss) and predictions (for printing and easy later evaluation) whether or not using teacher forcing.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Pseudocode for Decoder\n",
    "\n",
    "--without teacher forcing---\n",
    "initial_input = Variable ( tensor([[SOS]]) # SOS in Corpus\n",
    "inputs = [embed(initial_input)]\n",
    "hidden_states = list of size (n_layers,) of 0-tensors (wrapped as Variables)\n",
    "for i in 0..len(input_sequence)-1:  \n",
    "  inputs[i], hidden_states[0]  --first GRUCell--> hidden_states[0]\n",
    "  hidden_states[0], hidden_states[1] --second GRUCell--> hidden_states[1]\n",
    "  ... n_layers times    \n",
    "\n",
    "  apply Linear and torch.SoftMax to hidden state to get the probabilities (should be n_english)\n",
    "  max index of probabilities --> prediction\n",
    "  if prediction = target_corpus.EOS:\n",
    "      break\n",
    "  create a tensor from prediction and wrap as a Variable\n",
    "  prediction --embed--> next_input\n",
    "  append next_input to inputs\n",
    "  if next_input = EOS:\n",
    "    break\n",
    "return probabilities, predictions\n",
    "\n",
    "-- with teacher forcing --\n",
    "instead of [embed(initial_input)] use the embedding of source_corpus.SOS\n",
    "  and the second through last reference words\n",
    "don't break at EOS\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, target_vocab_size, n_layers=2, max_target_length=30):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = hidden_size\n",
    "        self.vocab_size = target_vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.max_length = max_target_length\n",
    "        \n",
    "        # initialize n_layers cells of type nn.GRUCell, an nn.Embedding like before\n",
    "        # initialize a Linear module and nn.ModuleList\n",
    "        \n",
    "    def forward(self, context, target_variable=None):\n",
    "        # if meant to teacher forcing include the target_variable\n",
    "        use_teacher_forcing = target_variable or None\n",
    "        \n",
    "        # return predictions as well for easier sampling\n",
    "\n",
    "\"\"\"\n",
    "#### old/my own code ####\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, target_vocab_size, n_layers=2, max_target_length=30):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = hidden_size\n",
    "        self.vocab_size = target_vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.max_length = self.max_target_length\n",
    "        \n",
    "        # initialize n_layers cells of type nn.GRUCell, an nn.Embedding like before\n",
    "        # initialize a Linear module and nn.ModuleList\n",
    "        \n",
    "    def forward(self, context, target_variable=None):\n",
    "        # if meant to teacher forcing include the target_variable\n",
    "        use_teacher_forcing = target_variable or None\n",
    "        \n",
    "        # return predictions as well for easier sampling\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads file and removes all non a-z characters\n",
    "def read_file(text_file):\n",
    "    contents = list(open(text_file, \"r\"))\n",
    "    acceptable = range(65, 91) + range(97, 123) + [32]\n",
    "    contents = [filter(lambda x:ord(x) in acceptable, line).strip().lower() \\\n",
    "                for line in contents]\n",
    "    contents = [\" \".join(re.split(r\"\\s+\", line)) for line in contents] # removes double spaces\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters needed for data processing\n",
    "n_test = 3000\n",
    "max_seq_len = 30\n",
    "\n",
    "## data preprocessing ##\n",
    "source_file = \"data/es.txt\"\n",
    "target_file = \"data/en.txt\"\n",
    "\n",
    "source_lines = read_file(source_file)\n",
    "target_lines = read_file(target_file)\n",
    "\n",
    "n_words = lambda s:len(s.split(\" \"))\n",
    "keep_pair_if = lambda pair: max(n_words(pair[0]),n_words(pair[1])) < max_seq_len\n",
    "\n",
    "# filter out source/reference pairs that exceed max_seq_len\n",
    "pairs = zip(source_lines, target_lines)\n",
    "pairs = filter(keep_pair_if, pairs)\n",
    "\n",
    "# training_pairs = filter(keep_pair_if, training_pairs)\n",
    "training_pairs = filter(keep_pair_if, pairs)\n",
    "source_lines, target_lines = zip(*training_pairs)\n",
    "\n",
    "source_corpus = Corpus(source_lines)\n",
    "target_corpus = Corpus(target_lines)\n",
    "n_spanish = source_corpus.corpus_size\n",
    "n_english = target_corpus.corpus_size\n",
    "all_indexed_pairs = zip(source_corpus.training, target_corpus.training)\n",
    "\n",
    "np.random.seed(2)\n",
    "test_idc = randint(0, len(training_pairs), n_test)\n",
    "train_idc = set(np.arange(len(pairs))) - set(test_idc)\n",
    "\n",
    "# list of 2-string tuples consisting of source / reference sentence pairs\n",
    "testing_pairs = map(lambda k: pairs[k], test_idc)\n",
    "training_pairs = map(lambda k: pairs[k], train_idc)\n",
    "\n",
    "# list of tuples consisting of source / reference sentence pairs \n",
    "# but represented by word indexes\n",
    "train_index_pairs = map(lambda k:all_indexed_pairs[k], train_idc)\n",
    "test_index_pairs = map(lambda k:all_indexed_pairs[k], test_idc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you will use this class later, consider using ipython terminal\n",
    "# to get used to initializing Parameters, Variables\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # use rand(n) to get tensors to initialize your weight matrix and bias tensor \n",
    "        # then use Parameter( ) to wrap them as Variables visible to module.parameters()\n",
    "        # consider xavier dense initialization (the one used without relu)\n",
    "        self.weights = Variable(torch.rand(output_size, input_size), requires_grad=True)\n",
    "        self.bias = Variable(torch.randn(output_size), requires_grad=True)        \n",
    "        \n",
    "    def forward(self, input_var, use_relu=False):\n",
    "        # standard linear layer, just an affine transform with no nonlinearity \n",
    "        #  for this lab\n",
    "        # use torch.matmul not mm\n",
    "        return torch.matmul(self.weights, input_var) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# possible hyperparameters\n",
    "epoch_length = 4000 # can use all sentences\n",
    "batch_size = 20     # should divide epoch_length\n",
    "n_layers = 2        # 2 or more recommended\n",
    "learning_rate = .005\n",
    "decay_rate = .85 ** (1./epoch_length)\n",
    "print_every = batch_size  # good practice: should divide epoch_length\n",
    "n_epochs = 30\n",
    "hidden_size = 500\n",
    "teacher_forcing_ratio = .5\n",
    "\n",
    "# def __init__(self, hidden_size, source_vocab_size, n_layers=2):\n",
    "encoder = Encoder(hidden_size, n_spanish, n_layers)\n",
    "# def __init__(self, hidden_size, target_vocab_size, n_layers=2, max_target_length=30):\n",
    "decoder = Decoder(hidden_size, n_english, n_layers, max_target_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif it's not working\\n-tune hyperparameters\\n-batch_size, teacher forcing, learning rate are good places to start\\n-Make sure your teacher forcing implementation is correct\\n-Try setting max_norm to 1 and scale_grad_by_freq=True when initializing both embeddings\\n-if scale_grad_by_freq=True concatenate Variables before embedding whenever possible\\n-Consult Sutskever's 2014 paper\\n\\nTraining philosophy behind vanilla seq2seq nmt systems (see also sutskever, 2014):\\n-need to learn somewhat one to one word connections first\\n-hift to learning long term dependencies \\n-consider a schedule for learning rate and/or teacher forcing ratio\\n\\nSaving and restoring weights might speed up your workflow significantly\\n-might want to checkpoint once you've learned word-word connections\\n\\nRegularization is super simple in pytorch.\\n-Be careful with doing dropout on the hidden states between cells\\n  see https://arxiv.org/pdf/1512.05287.pdf.\\n-low to moderate dropout after both embeddings may be helpful\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss(output_probs, correct_indices, predicted_indexes):#, predicted_sentence):\n",
    "    \"\"\" \n",
    "    params:\n",
    "      output_probs: a list of Variable (not FloatTensor)\n",
    "      with the predicted sequence length\n",
    "      correct_indices: a list or tensor of type int with the same length, will need\n",
    "                       to be converted to a Variable before compared to the output_probs\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Convert both output_probs and correct_indices to Variables\n",
    "    # output_probs should have one more dimension than correct_indices\n",
    "    # Use NLLoss to compute cross entropy without taking softmax twice\n",
    "    # should return a variable representing the loss    \n",
    "    # see regularization notes below\n",
    "    pass\n",
    "\n",
    "\n",
    "def print_output(teacher_forced, source_indices, predicted_indices, reference_indices, iteration, loss_info=None):\n",
    "    global source_corpus, target_corpus\n",
    "    if teacher_forced:\n",
    "        print(\"\\niteration %d: using teacher forcing\" % iteration)\n",
    "    else:\n",
    "        print(\"\\niteration %d\" % iteration)\n",
    "    print (\"In:       \", source_corpus.index_to_sentence(source_indices))\n",
    "    print (\"Out:      \", target_corpus.index_to_sentence(predicted_indices))\n",
    "    print (\"Reference:\", target_corpus.index_to_sentence(predicted_indices))\n",
    "\n",
    "    \n",
    "\n",
    "def train(encoder, decoder, training_pairs, testing_pairs, \n",
    "                source_corpus, target_corpus, teacher_forcing_ratio, \n",
    "                epoch_size, learning_rate, decay, batch_size, print_every):\n",
    "    \"\"\"\n",
    "    You may want to lower the teacher forcing ratio as the number \n",
    "      of epochs progresses as it starts to learn word-word connections.\n",
    "    \n",
    "    In PyTorch some optimizers don't allow for decaying learning rates\n",
    "    -Adam does however\n",
    "    -however initializing new optimizers is trivial\n",
    "    -You may want to use a learning rate schedule instead of decay\n",
    "    \"\"\"\n",
    "        \n",
    "    # initialize the optimizer(s) using both the encoder's and decoder's parameters\n",
    "    \n",
    "    batched_loss = 0\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(epoch_size):\n",
    "            use_teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # consider whether or not to use teacher forcing on printing iterations\n",
    "            # use_teacher_forcing = use_teacher_forcing or (i % print_every == 0)\n",
    "            \n",
    "            source, reference = training_pairs[randint(0, len(training_pairs))]\n",
    "            # convert source and reference to Tensors then Variables (use type Long not Int)\n",
    "            \n",
    "            # run source_var through the encoder\n",
    "            # run the context vector through the decoder\n",
    "            # if use_teacher_forcing include the reference variable when calling your decoder\n",
    "            \n",
    "            # feed the output probabilities and the reference sentence's indices to the loss\n",
    "            #   where it will do cross entropy using NLLLoss\n",
    "            \n",
    "            # loss = get_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # implement the batch update as shown in the spec\n",
    "            \n",
    "            \n",
    "            if (j+1) % print_every == 0:\n",
    "                print_output(use_teacher_forcing, source_idc, predicted, target_idc)\n",
    "                                \n",
    "        # run test iteration, print loss, accuracy, perplexity\n",
    "        \n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "# use this to print out your final translation sentences \n",
    "def sample(encoder, decoder, source_sentences, reference_sentences):#, testing_results = None):\n",
    "    for source, reference in zip(source_sentences, reference_sentences):\n",
    "        source = Variable(LongTensor(source), volatile=True)\n",
    "        # volatile means that the computation graph does not accumulate\n",
    "        # never use volatile except at inference time, any leaf nodes with \n",
    "        #   will cause the computational graph to not accumulate\n",
    "        # run source through your encoder and decoder and get the predicted sentence\n",
    "        \n",
    "\n",
    "\n",
    "# encoder, decoder = train(...)\n",
    "\n",
    "# print out 100 samples from your test set using sample\n",
    "\n",
    "\n",
    "# elective, may find this helpful if not using a GPU\n",
    "def save_weights(encoder, decoder):\n",
    "    params = list(encoder.named_parameters()) + list(decoder.named_parameters())\n",
    "    params_np = [(p[0], p[1].data.cpu().numpy()) for p in params]\n",
    "    for name, param in params_np:\n",
    "        pass # see np.save(name, param) or pickle\n",
    "\n",
    "def load_weights(encoder, decoder):\n",
    "    # load ndarrays from disk, convert them to tensors\n",
    "    params = list(encoder.named_parameters()) + list(decoder.named_parameters())\n",
    "    for p, t in zip(params, tensors):\n",
    "        name, param = p\n",
    "        param.data.set_(t)\n",
    "     \n",
    "\"\"\"\n",
    "if it's not working\n",
    "-tune hyperparameters\n",
    "-batch_size, teacher forcing, learning rate are good places to start\n",
    "-Make sure your teacher forcing implementation is correct\n",
    "-Try setting max_norm to 1 and scale_grad_by_freq=True when initializing both embeddings\n",
    "-if scale_grad_by_freq=True concatenate Variables before embedding whenever possible\n",
    "-Consult Sutskever's 2014 paper\n",
    "\n",
    "Training philosophy behind vanilla seq2seq nmt systems (see also sutskever, 2014):\n",
    "-need to learn somewhat one to one word connections first\n",
    "-hift to learning long term dependencies \n",
    "-consider a schedule for learning rate and/or teacher forcing ratio\n",
    "\n",
    "Saving and restoring weights might speed up your workflow significantly\n",
    "-might want to checkpoint once you've learned word-word connections\n",
    "\n",
    "Regularization is super simple in pytorch.\n",
    "-Be careful with doing dropout on the hidden states between cells\n",
    "  see https://arxiv.org/pdf/1512.05287.pdf.\n",
    "-low to moderate dropout after both embeddings may be helpful\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
