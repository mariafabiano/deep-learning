{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from textloader import TextLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops.rnn_cell import BasicLSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.python.ops.rnn_cell import RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mygru( RNNCell ):\n",
    " \n",
    "    def __init__( self, num_units, input_size=None):\n",
    "        self.num_units = num_units\n",
    "        self.input_size = input_size\n",
    " \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.num_units\n",
    " \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.num_units\n",
    " \n",
    "    def __call__( self, inputs, state, scope=None ):\n",
    "        # h should be [batch_size, num_units]\n",
    "        # state should be [batch_size, num_units]\n",
    "        with tf.variable_scope(\"GRU_cell\") as scope:\n",
    "            init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            Wz = tf.get_variable(\"W_z\", [inputs.get_shape()[1], self.num_units], initializer=init)\n",
    "            Uz = tf.get_variable(\"U_z\", [state.get_shape()[1], self.num_units], initializer=init)\n",
    "            bz = tf.get_variable(\"b_z\", [self.num_units], initializer=init)\n",
    "            zt = tf.sigmoid(tf.nn.bias_add(tf.matmul(inputs, Wz) + tf.matmul(state, Uz), bz)) # tf.nn.bias_add() ??\n",
    "            Wr = tf.get_variable(\"W_r\", [inputs.get_shape()[1], self.num_units], initializer=init)\n",
    "            Ur = tf.get_variable(\"U_r\", [state.get_shape()[1], self.num_units], initializer=init)\n",
    "            br = tf.get_variable(\"b_r\", [self.num_units], initializer=init)\n",
    "            rt = tf.sigmoid(tf.nn.bias_add(tf.matmul(inputs, Wr) + tf.matmul(state, Ur), br))\n",
    "            Wh = tf.get_variable(\"W_h\", [inputs.get_shape()[1], self.num_units], initializer=init)\n",
    "            Uh = tf.get_variable(\"U_h\", [state.get_shape()[1], self.num_units], initializer=init)\n",
    "            bh = tf.get_variable(\"b_h\", [self.num_units], initializer=init)\n",
    "            intermediate = tf.nn.bias_add(tf.matmul(inputs, Wh) + tf.matmul(tf.multiply(rt, state), Uh), bh)\n",
    "            ht = tf.multiply(zt, state) + tf.multiply(1.-zt, tf.tanh(intermediate))\n",
    "            return ht, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "batch_size = 50\n",
    "sequence_length = 50\n",
    "state_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "data_loader = TextLoader( \".\", batch_size, sequence_length )\n",
    "vocab_size = data_loader.vocab_size  # dimension of one-hot encodings\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "\n",
    "# define placeholders for our inputs.  \n",
    "# in_ph is assumed to be [batch_size, sequence_length]\n",
    "# targ_ph is assumed to be [batch_size, sequence_length]\n",
    "\n",
    "in_ph = tf.placeholder( tf.int32, [ batch_size, sequence_length ], name='inputs' )\n",
    "targ_ph = tf.placeholder( tf.int32, [ batch_size, sequence_length ], name='targets' )\n",
    "in_onehot = tf.one_hot( in_ph, vocab_size, name=\"input_onehot\" )\n",
    "\n",
    "inputs = tf.split( in_onehot, sequence_length, axis=1 )\n",
    "inputs = [tf.squeeze(i, [1]) for i in inputs] # inputs is list length sequence_length; each element is [batch_size, vocab_size]\n",
    "targets = tf.split( targ_ph, sequence_length, axis=1, ) # targets is list length sequence_length; each element of targets is 1D vector length batch_size\n",
    "\n",
    "# ------------------\n",
    "# COMPUTATION GRAPH\n",
    "\n",
    "with tf.variable_scope(\"Computation_Graph\") as scope:\n",
    "    basic1 = mygru(state_dim) # If I weren't using my own GRU class, I would call BasicLSTMCell here instead\n",
    "    basic2 = mygru(state_dim) # If I weren't using my own GRU class, I would call BasicLSTMCell here instead\n",
    "    rnn = MultiRNNCell([basic1,basic2])\n",
    "    initial_state = rnn.zero_state(batch_size, tf.float32) # initial_state is a list of tensors\n",
    "    output_list, final_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, rnn)\n",
    "    \n",
    "    init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    W = tf.Variable(tf.random_normal([state_dim, vocab_size], stddev=0.02))\n",
    "    b = tf.Variable(tf.random_normal([vocab_size], stddev=0.01))\n",
    "    logits = [tf.matmul(output, W) + b for output in output_list]\n",
    "\n",
    "    weights = [1.]*sequence_length\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss(logits, targets, weights)\n",
    "\n",
    "    c = 0.001\n",
    "    optim = tf.train.AdamOptimizer(learning_rate=c).minimize(loss)\n",
    "    \n",
    "# ------------------\n",
    "# SAMPLER GRAPH\n",
    "\n",
    "# reuse the parameters of the cell and the parameters you used to transform state outputs to logits!\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 1\n",
    "s_in_ph = tf.placeholder( tf.int32, [batch_size], name='inputs' )\n",
    "s_in_onehot = tf.one_hot( s_in_ph, vocab_size, name=\"input_onehot\" )\n",
    "\n",
    "s_inputs = tf.split( s_in_onehot, sequence_length, axis=0 )\n",
    "\n",
    "with tf.variable_scope(\"Sampler\") as scope:\n",
    "    s_initial_state = rnn.zero_state(batch_size, tf.float32) # initial_state is a list of tensors\n",
    "\n",
    "    s_output_list, s_final_state = tf.contrib.legacy_seq2seq.rnn_decoder(s_inputs, s_initial_state, rnn)\n",
    "    \n",
    "    s_probs = [tf.matmul(output, W) + b for output in s_output_list] # W, b from previous RNN\n",
    "\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "\n",
    "def sample( num=200, prime='ab' ):\n",
    "    # prime the pump \n",
    "\n",
    "    # generate an initial state. this will be a list of states, one for\n",
    "    # each layer in the multicell.\n",
    "    s_state = sess.run( s_initial_state )\n",
    "\n",
    "    # for each character, feed it into the sampler graph and\n",
    "    # update the state.\n",
    "    for char in prime[:-1]:\n",
    "        x = np.ravel( data_loader.vocab[char] ).astype('int32')\n",
    "        feed = { s_in_ph:x }\n",
    "        for i, s in enumerate( s_initial_state ):\n",
    "            feed[s] = s_state[i]\n",
    "        s_state = sess.run( s_final_state, feed_dict=feed )\n",
    "\n",
    "    # now we have a primed state vector; we need to start sampling.\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in xrange(num):\n",
    "        x = np.ravel( data_loader.vocab[char] ).astype('int32')\n",
    "\n",
    "        # plug the most recent character in...\n",
    "        feed = { s_in_ph:x }\n",
    "        for i, s in enumerate( s_initial_state ):\n",
    "            feed[s] = s_state[i]\n",
    "        ops = [s_probs]\n",
    "        ops.extend( list(s_final_state) )\n",
    "\n",
    "        retval = sess.run( ops, feed_dict=feed )\n",
    "\n",
    "        s_probsv = retval[0]\n",
    "        s_state = retval[1:]\n",
    "\n",
    "        # ...and get a vector of probabilities out!\n",
    "\n",
    "        # now sample (or pick the argmax)\n",
    "        sample = np.argmax( s_probsv[0] )\n",
    "        # sample = np.random.choice( vocab_size, p=s_probsv[0] )\n",
    "\n",
    "        pred = data_loader.chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "\n",
    "    return ret\n",
    "\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run( tf.global_variables_initializer() )\n",
    "summary_writer = tf.summary.FileWriter( \"./tf_logs\", graph=sess.graph )\n",
    "print \"FOUND %d BATCHES\" % data_loader.num_batches\n",
    "\n",
    "for j in xrange(200):\n",
    "\n",
    "    state = sess.run( initial_state )\n",
    "    data_loader.reset_batch_pointer()\n",
    "\n",
    "    for i in xrange( data_loader.num_batches ):\n",
    "        \n",
    "        x,y = data_loader.next_batch()\n",
    "\n",
    "        # we have to feed in the individual states of the MultiRNN cell\n",
    "        feed = { in_ph: x, targ_ph: y }\n",
    "        for k, s in enumerate( initial_state ):\n",
    "            feed[s] = state[k]\n",
    "\n",
    "        ops = [optim,loss]\n",
    "        ops.extend( list(final_state) )\n",
    "\n",
    "        # retval will have at least 3 entries:\n",
    "        # 0 is None (triggered by the optim op)\n",
    "        # 1 is the loss\n",
    "        # 2+ are the new final states of the MultiRNN cell\n",
    "        retval = sess.run( ops, feed_dict=feed )\n",
    "\n",
    "        lt = retval[1]\n",
    "        state = retval[2:]\n",
    "\n",
    "        if i%1000==0:\n",
    "            print \"%d %d\\t%.4f\" % ( j, i, lt )\n",
    "\n",
    "    print sample( num=60, prime=\"He \" )\n",
    "#    print sample( num=60, prime=\"ababab\" )\n",
    "#    print sample( num=60, prime=\"foo ba\" )\n",
    "#    print sample( num=60, prime=\"abcdab\" )\n",
    "\n",
    "with open(\"my_hp.txt\", 'w') as f:\n",
    "    for i in xrange(20):\n",
    "        f.write(sample(num=60, prime=\"He \") + '\\n')\n",
    "\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harry Potter and the Deathly Hallows, Chapter 36 (my text choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was shathere was a forced of the wand him and and the crowd \n",
      "He was beend atound his eyes at the said to the comporse he had\n",
      "He was beend atound his eyes at the sale was and and the contan\n",
      "He was because he loved her and couts to to kill them war arryo\n",
      "He was she chanted into the air. “I’s dows donged loughter, and\n",
      "He was a forted of her and cors any had no was streamed of the \n",
      "He was seath of the samper that the same to the wand was a fore\n",
      "He was shathere with the had not lise that the wand in the had \n",
      "He was shathere wished a filll the castle later him him him wit\n",
      "He was shathere wished a for like a might and harde to his feet\n",
      "He was a forseath of him a finet time, the flain a chanted onte\n",
      "He was a forced of the Elder Wand like a combort of them and ha\n",
      "He was slake’s had been it and the same siled the same siled an\n",
      "He was sidenst from the castle and streamed Harry as the same s\n",
      "He was shathered the entrang the granted and whise to dist they\n",
      "He was nower at the wand in the compone, and the screamed Harry\n",
      "He was now there was as and a corlare, but he dever you dur wor\n",
      "He was strokish flooring Harry whe had been had ging to dear, t\n",
      "He was a fresh outhin sitelased and curses and and them all of \n",
      "He was a fresh outhin\n",
      "sutellded arress that were mister of thei\n",
      "He was stroking the spot, but then it was as Harry was andred. \n",
      "He was now tree hem loved cone po time foo to touth the sounde\n"
     ]
    }
   ],
   "source": [
    "with open(\"my_hp.txt\", 'r') as f:\n",
    "    hp_lines = f.readlines()\n",
    "for h in hp_lines:\n",
    "    print(h[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And because of the words of God we will repent and see that ye s\n",
      "And because of the words of Amulek, and the word of God with the\n",
      "And because of their words, and their country to the land of Mid\n",
      "And Amulek had gone forth against themselves to the land of Zara\n",
      "And Amulek, being lifted up in the land of Middoni and the serva\n",
      "And of every kind.\n",
      "\n",
      " But behold, it was not all things, and the \n",
      "And of every kind; and they were all an athye over the land of S\n",
      "And of the servants of the Lamanites who were in the land of Mid\n",
      "And of the seed.\n",
      "\n",
      " And it came to pass that the Lamanites are a \n",
      "And of the servants of the Lamanites who were slain; for behold,\n",
      "And of the servants of the Lamanites who were in the land of Mid\n",
      "And of the Lamanites who had been led by the seashore, and the p\n",
      "And of the traditions of their fathers, and the men of the Laman\n",
      "And of the Lord were goodness.\n",
      "\n",
      " And now, my son, I desire that \n",
      "And of the Lord were good shercy hath great presence of the Lama\n",
      "And of the seed.\n",
      "\n",
      " And they were angry with Amulek, and the stre\n",
      "And of the Lord were goodness.\n",
      "\n",
      " And now behold, this is my joy \n",
      "And of the Lamanites who had been converted unto the Lord which \n",
      "And of those who have been slain.\n",
      "\n",
      " And now behold, I say unto y\n",
      "And of the Lamanites which they had seen all those who were all \n"
     ]
    }
   ],
   "source": [
    "with open(\"my_alma.txt\", 'r') as f:\n",
    "    hp_lines = f.readlines()\n",
    "for h in hp_lines:\n",
    "    print(h[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
